{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FakeNewsDetection_Glove_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmat/fakenewsdetection/blob/main/FakeNewsDetection_Glove_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XY7z2AWVqqQS",
        "outputId": "5fd216a0-34de-4dfe-bc83-ebe2e09798b4"
      },
      "source": [
        "!pip install torchtext==0.8.0\n",
        "!pip install torch==1.7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/23/8499af6d9c22b29b01f66a2c11d38ce71cd1cafa2655913c29818ed4a00f/torchtext-0.8.0-cp36-cp36m-manylinux1_x86_64.whl (6.9MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9MB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (1.18.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (1.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.8.0) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (3.0.4)\n",
            "Installing collected packages: torchtext\n",
            "  Found existing installation: torchtext 0.7.0\n",
            "    Uninstalling torchtext-0.7.0:\n",
            "      Successfully uninstalled torchtext-0.7.0\n",
            "Successfully installed torchtext-0.8.0\n",
            "Collecting torch==1.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/2a/58f8078744e0408619c63148f7a2e8e48cf007e4146b74d4bb67c56d161b/torch-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (776.7MB)\n",
            "\u001b[K     |████████████████████████████████| 776.7MB 20kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.7) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.7) (0.16.0)\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.6.0\n",
            "    Uninstalling torch-1.6.0:\n",
            "      Successfully uninstalled torch-1.6.0\n",
            "Successfully installed torch-1.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfR9Mim9reIe"
      },
      "source": [
        "# For local run\n",
        "data_path = 'data/'"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9NutMxSzHPw",
        "outputId": "708dfbbc-75ff-4a64-ef27-b4f370a082dd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_path = '/content/drive/My Drive/SML/fake news detection/'"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp2KO5KPWRU8"
      },
      "source": [
        "# Libraries\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "#deal with tensors\n",
        "import torch   \n",
        "\n",
        "#handling text data\n",
        "from torchtext import data \n",
        "\n",
        "# Evaluation\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "import seaborn as sns"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge5pwMsoJVjt"
      },
      "source": [
        "#Reproducing same results\n",
        "SEED = 2019\n",
        "\n",
        "#Torch\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "#Cuda algorithms\n",
        "torch.backends.cudnn.deterministic = True  "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf6koevxtIsF"
      },
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = 'cpu'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvxcOiKB2mwP",
        "outputId": "39358650-2275-4fa1-a272-a61d906e69b3"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Dec 17 20:13:19 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    24W / 300W |     10MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3OmSuBlURs-"
      },
      "source": [
        "import string\n",
        "def remove_punctuation(x):\n",
        "  aux = ''.join(ch for ch in x if ch not in set(string.punctuation))\n",
        "  return aux"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaAKKFFRUzTy"
      },
      "source": [
        "# load the training and testing data\n",
        "data_train = pd.read_csv(data_path + 'fnn_train.csv')\n",
        "data_test = pd.read_csv(data_path + 'fnn_test.csv')\n",
        "data_val = pd.read_csv(data_path + 'fnn_dev.csv')"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZlj78a8VsuI"
      },
      "source": [
        "# convert the data labels into categorical variables (real = 0, fake = 1)\n",
        "data_train[\"label_fnn\"] = data_train[\"label_fnn\"].replace(\"real\",0)\n",
        "data_train[\"label_fnn\"] = data_train[\"label_fnn\"].replace(\"fake\",1)\n",
        "data_test[\"label_fnn\"] = data_test[\"label_fnn\"].replace(\"real\",0)\n",
        "data_test[\"label_fnn\"] = data_test[\"label_fnn\"].replace(\"fake\",1)\n",
        "data_val[\"label_fnn\"] = data_val[\"label_fnn\"].replace(\"real\",0)\n",
        "data_val[\"label_fnn\"] = data_val[\"label_fnn\"].replace(\"fake\",1)\n",
        "\n",
        "data_train = data_train.drop(\"date\", axis=1)\n",
        "data_test = data_test.drop(\"date\", axis=1)\n",
        "data_val = data_val.drop(\"date\", axis=1)\n",
        "data_train = data_train.drop([\"speaker\"],axis=1)\n",
        "data_test = data_test.drop([\"speaker\"],axis=1)\n",
        "data_val = data_val.drop([\"speaker\"],axis=1)\n",
        "data_train_csv = data_train.drop([\"sources\"],axis=1)\n",
        "data_test_csv = data_test.drop([\"sources\"],axis=1)\n",
        "data_val_csv = data_val.drop([\"sources\"],axis=1)\n",
        "data_train_csv = data_train_csv.rename(columns={\"fullText_based_content\": \"text\",\"label_fnn\": \"label\"}, errors=\"raise\")\n",
        "data_test_csv = data_test_csv.rename(columns={\"fullText_based_content\": \"text\",\"label_fnn\": \"label\"}, errors=\"raise\")\n",
        "data_val_csv = data_val_csv.rename(columns={\"fullText_based_content\": \"text\",\"label_fnn\": \"label\"}, errors=\"raise\")\n",
        "\n",
        "#Remove punctuations\n",
        "data_train_csv['text'] = data_train_csv['text'].apply(remove_punctuation)\n",
        "data_test_csv['text'] = data_test_csv['text'].apply(remove_punctuation)\n",
        "data_val_csv['text'] = data_val_csv['text'].apply(remove_punctuation)\n",
        "\n",
        "data_train_csv.to_csv(data_path + r'fnn_train_clean.csv', index = False)\n",
        "data_test_csv.to_csv(data_path + r'fnn_test_clean.csv', index = False)\n",
        "data_val_csv.to_csv(data_path + r'fnn_val_clean.csv', index = False)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCHj8215KL3G",
        "outputId": "1ab92607-d7c9-44eb-9d2a-6994acd02978"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfkyvYbvuvQU"
      },
      "source": [
        "\n",
        "LABEL = data.LabelField(dtype = torch.float) \n",
        "#For LSTM\n",
        "TEXT = data.Field(tokenize='spacy', include_lengths=True, batch_first=True, lower=True,  stop_words = stop_words)\n",
        "\n",
        "#For NN with Bigram\n",
        "# TEXT = data.Field(tokenize='spacy', lower=True, preprocessing = generate_bigrams,  stop_words = stop_words)\n",
        "\n",
        "fields = [('id:',None), ('statement:',None), ('paragraph_based_content',None),\n",
        "      ('text', TEXT),('label',LABEL)]"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmG5JnzBv3cn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d18afb0-f26e-4260-ff7c-c0e909995e40"
      },
      "source": [
        "news_data = data.TabularDataset(\n",
        "        path= data_path+ \"fnn_train_clean.csv\", \n",
        "        format=\"CSV\", \n",
        "        fields=fields,\n",
        "        skip_header=True)\n",
        "\n",
        "print(vars(news_data.examples[1]))"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['supreme', 'court', 'justices', 'embarked', 'three', 'days', 'oral', 'arguments', 'historic', 'lawsuit', 'health', 'care', 'law', 'gov', 'rick', 'scott', 'went', 'national', 'tv', 'media', 'blitz', 'said', 'one', 'regurgitated', 'falsehoods', 'health', 'care', 'debate', '\\n', 'ran', 'campaign', 'getting', 'state', 'back', 'work', 'biggest', 'jobkiller', 'ever', 'said', 'fox', 'friends', 'march', '26', '2012', 'mean', 'think', 'government', 'ca', 'n’t', 'buy', 'health', 'care', 'cheaper', 'anybody', 'else', 'unbelievable', 'penalties', 'go', '\\n', 'big', 'jobkiller', 'cost', 'much', 'said', '\\n', 'politifact', 'examined', 'similar', 'claims', 'law', '’s', 'jobkilling', 'effect', 'house', 'republican', 'leader', 'eric', 'cantor', ' ', 'us', 'chamber', 'commerce', 'former', 'us', 'senate', 'candidate', 'former', 'florida', 'house', 'majority', 'leader', 'adam', 'hasner', ' ', 'none', 'fared', 'well', 'truthometer', '\\n', 'wanted', 'rule', 'scott', '’s', 'statement', 'given', 'renewed', 'debate', 'law', '’ll', 'explain', 'examined', 'jobkilling', 'rhetoric', 'past', 'getting', 'around', 'scott', 'specifically', '\\n', 'none', 'folks', 'made', 'claim', 'could', 'back', 'valid', 'projections', 'job', 'losses', 'instead', 'presented', 'partisan', 'reports', 'skewed', 'interpretations', 'independent', 'reports', 'evidence', '\\n', 'one', 'republican', 'document', 'obamacare', 'budgetbusting', 'jobkilling', 'health', 'care', 'law', 'claimed', 'independent', 'analyses', 'determined', 'health', 'care', 'law', 'cause', 'significant', 'job', 'losses', 'us', 'economy', 'cites', '2010', 'report', 'cbo', 'analyzes', 'impact', 'legislation', 'allegedly', 'determined', 'law', 'would', 'lead', 'roughly', '650000', 'lost', 'jobs', '\\n', 'report', 'n’t', 'say', '\\n', 'said', 'reduction', 'amount', 'labor', 'economy', 'would', 'roughly', 'half', 'percent', ' ', 'onerous', 'regulation', 'workers', 'around', 'retirement', 'age', 'may', 'decide', 'stop', 'working', 'earlier', 'planned', 'report', 'states', 'pointing', 'affordability', 'insurance', 'offered', 'outside', 'workplace', '\\n', 'report', 'says', 'law', 'may', 'also', 'mean', 'people', 'seek', 'jobs', 'medicaid', 'expansion', 'allows', 'lowincome', 'people', 'work', 'still', 'qualify', 'program', '\\n', 'cbo', 'highlight', 'part', 'law', 'likely', 'lead', 'lost', 'jobs', 'requirement', 'businesses', '50', 'workers', 'pay', 'fee', 'offer', 'health', 'insurance', 'plan', 'offer', 'falls', 'short', 'criteria', 'least', 'one', 'employee', 'receives', 'subsidy', 'tobecreated', 'insurance', 'exchange', '\\n', 'fee', 'cbo', 'states', 'passed', 'employees', 'reduced', 'wages', 'compensation', 'businesses', 'pay', 'chunk', 'employees', 'minimum', 'wage', '’s', 'inevitable', 'take', 'fewer', 'lowwage', 'workers', 'may', 'also', 'respond', 'hiring', 'parttime', 'seasonal', 'employees', '\\n', '’s', 'specific', 'gets', 'cbo', '’s', 'update', 'take', 'time', 'assess', 'effects', 'law', 'parts', 'wo', 'n’t', 'implemented', 'another', 'couple', 'years', '\\n', 'another', 'source', 'republicans', 'used', 'backup', 'pretty', 'irrelevant', 'national', 'association', 'independent', 'business', 'said', '2009', 'report', 'impact', 'provision', 'requiring', 'businesses', 'offer', 'insurance', 'would', 'lead', 'elimination', '16', 'million', 'jobs', 'twothirds', 'would', 'small', 'business', 'blanket', 'employer', 'mandate', 'make', 'final', 'law', 'exempts', 'companies', '50', 'fewer', 'employees', 'mandate', '\\n', 'nfib', 'produced', 'recent', 'study', 'november', '2011', 'blasting', 'another', 'part', 'law', 'escalating', 'annual', 'fee', 'starting', '2014', 'health', 'insurance', 'sector', 'group', 'says', 'passed', 'businesses', 'increased', 'premiums', '\\n', 'requirement', 'lobbying', 'group', 'calls', 'health', 'insurance', 'tax', 'would', 'lead', 'lost', 'privatesector', 'jobs', '125000', '249000', '2010', 'group', 'said', 'job', 'losses', 'would', 'total', '4700', '2021', 'florida', '\\n', '’ve', 'found', 'problems', 'nfib', 'research', 'past', 'stories', 'starters', '’s', 'really', 'independent', 'ideological', 'sense', 'lobbying', 'group', 'opposes', 'policy', 'places', 'financial', 'burden', 'business', 'sued', 'health', 'care', 'law', 'supreme', 'court', '\\n', 'words', '’s', 'exactly', 'goto', 'source', 'objectivity', '\\n', 'still', 'dug', 'substance', 'claim', 'law', 'cost', 'hundreds', 'thousands', 'private', 'jobs', 'recent', 'check', 'jobkilling', 'claim', 'several', 'skeptical', 'experts', 'told', 'us', 'impact', 'would', 'big', 'pointing', 'employer', 'payments', 'law', 'small', 'nfib', '’s', 'research', 'n’t', 'factor', 'new', 'tax', 'credits', 'law', 'small', 'businesses', 'would', 'actually', 'lead', 'savings', 'premium', 'contributions', '\\n', 'rhetorical', 'hysteria', 'explicit', 'term', '‘', 'job', 'killer', '’', 'enough', 'make', 'one', 'despair', 'rational', 'public', 'debate', 'said', 'henry', 'aaron', 'senior', 'fellow', 'brookings', 'institution', 'february', 'email', 'politifact', '\\n', 'like', 'cbo', 'w', 'e’ve', 'cautioned', 'discussion', 'law', 'wo', 'n’t', 'mostly', 'based', 'speculation', 'even', 'parts', 'law', 'go', 'effect', '2014', 'barring', 'supreme', 'court', 'action', 'wo', 'n’t', 'know', 'drastic', 'effect', 'jobs', 'years', 'follow', '\\n', 'friends', 'factcheckorg', 'arrived', 'conclusions', 'jobkilling', 'threat', 'january', '2011', '\\n', 'scott', 'could', 'cite', 'new', 'compelling', 'evidence', 'wellworn', 'claim', '\\n', '’s', 'prediction', 'based', 'conversations', 'business', 'owners', 'job', 'creators', 'spokesman', 'brian', 'burgess', 'wrote', 'email', '\\n', 'n’t', 'elaborate', '\\n', 'analyzing', 'truthfulness', 'predictions', 'tricky', 'business', 'normally', 'prefer', 'delve', 'jobkilling', 'claim', 'widely', 'spread', 'carry', 'proof', '\\n', 'scott', '’s', 'spokesman', 'said', 'prediction', 'based', 'anecdotal', 'conversations', 'business', 'owners', 'say', '’s', 'one', 'steeped', 'credible', 'independent', 'evidence', ' ', '’s', 'like', 'scare', 'tactic', 'rule', 'claim', 'false'], 'label': '1'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDg0s0AOfr1v",
        "outputId": "37d75004-cae6-4ece-aef0-a508e91993bc"
      },
      "source": [
        "test_data = data.TabularDataset(\n",
        "        path= data_path + \"fnn_test_clean.csv\", \n",
        "        format=\"CSV\", \n",
        "        fields=fields,\n",
        "        skip_header=True)\n",
        "\n",
        "print(vars(test_data.examples[1]))"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['abcs', 'week', 'chairmen', 'republican', 'democratic', ' ', 'national', 'parties', 'debated', 'significance', 'republican', 'charles', 'djous', ' ', 'victory', 'hawaii', 'congressional', 'special', 'election', 'may', '22', '2010', ' ', ' ', 'first', 'time', 'republican', 'represent', 'hawaii', 'congress', 'since', '1991', '\\n', 'rnc', ' ', 'chairman', 'michael', 'steele', 'called', 'victory', 'significant', 'given', 'hawaiis', ' ', 'penchant', 'supporting', 'democrats', 'noted', 'state', ' ', 'birthplace', 'president', 'barack', 'obama', 'sorry', 'birthers', '\\n', ' ', 'course', 'dnc', 'chairman', 'tim', 'kaine', 'different', 'read', 'said', 'djous', ' ', 'victory', 'unique', 'circumstances', 'special', 'election', ' ', 'called', 'replace', 'resigning', 'rep', 'neil', 'abercrombie', 'regular', 'fall', 'election', '   ', 'political', 'mood', 'swing', 'hawaii', 'special', 'election', ' ', 'include', 'primary', 'meant', 'parties', 'could', 'field', 'multiple', 'candidates', ' ', 'three', 'main', 'contenders', 'two', 'democrats', 'djou', 'lone', ' ', 'republican', 'little', '40', 'percent', 'overall', 'vote', '\\n', ' ', 'november', 'election', 'one', 'democrat', 'one', 'republican', ' ', 'feel', 'confident', 'winning', 'race', 'kaine', 'said', '\\n', ' ', 'steele', 'countered', 'hawaii', 'nt', 'history', 'throwing', 'incumbents', ' ', 'office', '\\n', 'obvious', 'fact', 'incumbents', ' ', 'easier', 'road', 'reelection', 'challengers', 'since', '1964', 'less', '85', ' ', 'percent', 'incumbents', 'returned', 'us', 'house', 'according', ' ', 'center', 'responsive', 'politics', 'since', '1998', 'incumbents', ' ', 'reelection', 'rate', 'hovered', '94', '98', 'percent', 'house', ' ', 'numbers', 'senate', 'little', 'lower', ' ', '79', '96', 'percent', ' ', 'elections', 'since', '1998', '\\n', 'incumbents', 'even', 'easier', ' ', 'time', 'getting', 'reelected', 'americas', 'youngest', 'state', '\\n', 'since', 'hawaii', ' ', 'nt', 'incorporated', 'state', '1959', 'nt', 'lot', 'history', ' ', 'examine', 'since', 'hawaii', 'four', 'members', 'congress', 'two', ' ', 'senators', 'two', 'members', 'us', 'house', 'relatively', ' ', 'elections', 'scour', '\\n', 'fact', '5', 'people', 'represented', ' ', 'state', 'us', 'senate', 'two', 'sitting', 'senators', 'democrats', 'daniel', ' ', 'inouye', 'daniel', 'akaka', 'three', 'others', ' ', 'republican', 'hiram', 'fong', ' ', 'democrat', 'spark', 'matsunaga', 'democrat', 'oren', 'e', 'long', 'none', 'lost', ' ', 'reelection', 'campaign', 'senate', ' ', 'fong', 'long', 'retired', ' ', 'matsunaga', 'died', 'office', '\\n', 'hawaiis', 'two', 'congressional', 'house', ' ', 'districts', 'created', '1971', 'previously', 'seats', 'elected', ' ', 'large', '\\n', 'djou', 'represent', 'hawaiis', '1st', 'district', 'second', ' ', 'republican', 'ever', 'replaces', 'abercrombie', '10term', ' ', 'democrat', 'resigned', 'run', 'governor', 'states', '2nd', 'district', ' ', 'never', 'elected', 'republican', '\\n', 'incumbents', 'lost', '\\n', ' ', 'really', '\\n', '2nd', 'district', 'patsy', 'mink', 'daniel', 'akaka', 'ed', 'case', ' ', 'resigned', 'retired', 'order', 'run', 'another', 'office', 'mink', ' ', 'eventually', 'reelected', 'us', 'house', 'died', 'office', '\\n', ' ', '1st', 'district', 'spark', 'matsunaga', 'cecil', 'heftel', 'pat', 'saiki', ' ', 'abercrombie', 'also', 'resigned', 'run', 'another', 'office', '\\n', ' ', 'one', 'asterisk', 'abercrombie', 'first', 'elected', 'congress', '1986', ' ', 'special', 'election', 'like', 'djou', 'though', 'special', 'election', ' ', 'lost', 'partys', 'primary', 'run', 'full', 'twoyear', 'term', ' ', 'day', '\\n', 'counted', 'found', '50', 'elections', ' ', 'incumbent', 'hawaii', 'ballot', 'closest', 'anyone', 'got', 'losing', ' ', 'abercrombie', '1986', 'nt', 'actually', 'incumbent', ' ', 'lost', 'democratic', 'primary', '\\n', 'depending', 'count', ' ', 'puts', 'reelection', 'rate', 'hawaii', '98', 'percent', '100', 'percent', ' ', 'higher', 'national', 'average', 'period', '\\n', 'though', ' ', 'steele', 'talking', 'federal', 'races', 'also', 'checked', 'see', ' ', 'incumbent', 'hawaii', 'governor', 'ever', 'lost', 'bid', 'reelection', 'happened', ' ', 'since', 'hawaii', 'became', 'state', '1962', 'republican', 'william', 'quinn', ' ', 'lost', 'democrat', 'john', 'burns', '\\n', 'back', 'statement', 'rnc', ' ', 'chairman', 'steele', 'said', 'hawaiians', 'nt', 'history', ' ', 'throwing', 'incumbents', 'office', 'said', 'djous', 'election', ' ', 'congress', 'may', 'means', 'republicans', 'good', 'shot', 'keeping', ' ', 'seat', 'november', 'business', 'predicting', ' ', 'outcome', 'house', 'races', 'incumbent', 'ever', 'lost', 'november', ' ', 'congressional', 'election', 'hawaii'], 'label': '0'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLF5UXWy_wu2",
        "outputId": "0c800ab8-7fe8-4139-ca97-38ac512c560f"
      },
      "source": [
        "valid_data = data.TabularDataset(\n",
        "        path= data_path + \"fnn_val_clean.csv\", \n",
        "        format=\"CSV\", \n",
        "        fields=fields,\n",
        "        skip_header=True)\n",
        "\n",
        "print(vars(test_data.examples[1]))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['abcs', 'week', 'chairmen', 'republican', 'democratic', ' ', 'national', 'parties', 'debated', 'significance', 'republican', 'charles', 'djous', ' ', 'victory', 'hawaii', 'congressional', 'special', 'election', 'may', '22', '2010', ' ', ' ', 'first', 'time', 'republican', 'represent', 'hawaii', 'congress', 'since', '1991', '\\n', 'rnc', ' ', 'chairman', 'michael', 'steele', 'called', 'victory', 'significant', 'given', 'hawaiis', ' ', 'penchant', 'supporting', 'democrats', 'noted', 'state', ' ', 'birthplace', 'president', 'barack', 'obama', 'sorry', 'birthers', '\\n', ' ', 'course', 'dnc', 'chairman', 'tim', 'kaine', 'different', 'read', 'said', 'djous', ' ', 'victory', 'unique', 'circumstances', 'special', 'election', ' ', 'called', 'replace', 'resigning', 'rep', 'neil', 'abercrombie', 'regular', 'fall', 'election', '   ', 'political', 'mood', 'swing', 'hawaii', 'special', 'election', ' ', 'include', 'primary', 'meant', 'parties', 'could', 'field', 'multiple', 'candidates', ' ', 'three', 'main', 'contenders', 'two', 'democrats', 'djou', 'lone', ' ', 'republican', 'little', '40', 'percent', 'overall', 'vote', '\\n', ' ', 'november', 'election', 'one', 'democrat', 'one', 'republican', ' ', 'feel', 'confident', 'winning', 'race', 'kaine', 'said', '\\n', ' ', 'steele', 'countered', 'hawaii', 'nt', 'history', 'throwing', 'incumbents', ' ', 'office', '\\n', 'obvious', 'fact', 'incumbents', ' ', 'easier', 'road', 'reelection', 'challengers', 'since', '1964', 'less', '85', ' ', 'percent', 'incumbents', 'returned', 'us', 'house', 'according', ' ', 'center', 'responsive', 'politics', 'since', '1998', 'incumbents', ' ', 'reelection', 'rate', 'hovered', '94', '98', 'percent', 'house', ' ', 'numbers', 'senate', 'little', 'lower', ' ', '79', '96', 'percent', ' ', 'elections', 'since', '1998', '\\n', 'incumbents', 'even', 'easier', ' ', 'time', 'getting', 'reelected', 'americas', 'youngest', 'state', '\\n', 'since', 'hawaii', ' ', 'nt', 'incorporated', 'state', '1959', 'nt', 'lot', 'history', ' ', 'examine', 'since', 'hawaii', 'four', 'members', 'congress', 'two', ' ', 'senators', 'two', 'members', 'us', 'house', 'relatively', ' ', 'elections', 'scour', '\\n', 'fact', '5', 'people', 'represented', ' ', 'state', 'us', 'senate', 'two', 'sitting', 'senators', 'democrats', 'daniel', ' ', 'inouye', 'daniel', 'akaka', 'three', 'others', ' ', 'republican', 'hiram', 'fong', ' ', 'democrat', 'spark', 'matsunaga', 'democrat', 'oren', 'e', 'long', 'none', 'lost', ' ', 'reelection', 'campaign', 'senate', ' ', 'fong', 'long', 'retired', ' ', 'matsunaga', 'died', 'office', '\\n', 'hawaiis', 'two', 'congressional', 'house', ' ', 'districts', 'created', '1971', 'previously', 'seats', 'elected', ' ', 'large', '\\n', 'djou', 'represent', 'hawaiis', '1st', 'district', 'second', ' ', 'republican', 'ever', 'replaces', 'abercrombie', '10term', ' ', 'democrat', 'resigned', 'run', 'governor', 'states', '2nd', 'district', ' ', 'never', 'elected', 'republican', '\\n', 'incumbents', 'lost', '\\n', ' ', 'really', '\\n', '2nd', 'district', 'patsy', 'mink', 'daniel', 'akaka', 'ed', 'case', ' ', 'resigned', 'retired', 'order', 'run', 'another', 'office', 'mink', ' ', 'eventually', 'reelected', 'us', 'house', 'died', 'office', '\\n', ' ', '1st', 'district', 'spark', 'matsunaga', 'cecil', 'heftel', 'pat', 'saiki', ' ', 'abercrombie', 'also', 'resigned', 'run', 'another', 'office', '\\n', ' ', 'one', 'asterisk', 'abercrombie', 'first', 'elected', 'congress', '1986', ' ', 'special', 'election', 'like', 'djou', 'though', 'special', 'election', ' ', 'lost', 'partys', 'primary', 'run', 'full', 'twoyear', 'term', ' ', 'day', '\\n', 'counted', 'found', '50', 'elections', ' ', 'incumbent', 'hawaii', 'ballot', 'closest', 'anyone', 'got', 'losing', ' ', 'abercrombie', '1986', 'nt', 'actually', 'incumbent', ' ', 'lost', 'democratic', 'primary', '\\n', 'depending', 'count', ' ', 'puts', 'reelection', 'rate', 'hawaii', '98', 'percent', '100', 'percent', ' ', 'higher', 'national', 'average', 'period', '\\n', 'though', ' ', 'steele', 'talking', 'federal', 'races', 'also', 'checked', 'see', ' ', 'incumbent', 'hawaii', 'governor', 'ever', 'lost', 'bid', 'reelection', 'happened', ' ', 'since', 'hawaii', 'became', 'state', '1962', 'republican', 'william', 'quinn', ' ', 'lost', 'democrat', 'john', 'burns', '\\n', 'back', 'statement', 'rnc', ' ', 'chairman', 'steele', 'said', 'hawaiians', 'nt', 'history', ' ', 'throwing', 'incumbents', 'office', 'said', 'djous', 'election', ' ', 'congress', 'may', 'means', 'republicans', 'good', 'shot', 'keeping', ' ', 'seat', 'november', 'business', 'predicting', ' ', 'outcome', 'house', 'races', 'incumbent', 'ever', 'lost', 'november', ' ', 'congressional', 'election', 'hawaii'], 'label': '0'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "il4JBh0wwbBK"
      },
      "source": [
        "import random\n",
        "\n",
        "(train_data, valid_data)=news_data.split(split_ratio=[0.8,0.2], random_state = random.seed(SEED))\n",
        "\n",
        "# (len(train_data),len(valid_data))\n",
        "# train_data = news_data"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owd14QC4wts9"
      },
      "source": [
        "vocab_size = 25000\n",
        "\n",
        "#For 300D GloVe\n",
        "# TEXT.build_vocab(train_data, max_size = vocab_size, vectors = \"glove.42B.300d\", unk_init = torch.Tensor.normal_)  \n",
        "\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size = vocab_size, vectors = \"glove.6B.100d\", unk_init = torch.Tensor.normal_)  \n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "#No. of unique tokens in text\n",
        "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
        "\n",
        "#No. of unique tokens in label\n",
        "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
        "\n",
        "#Commonly used words\n",
        "print(TEXT.vocab.freqs.most_common(10))  \n",
        "\n",
        "#Word dictionary\n",
        "# print(TEXT.vocab.stoi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9flK9qFtw0ly"
      },
      "source": [
        "#set batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "#Load an iterator\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = lambda x: len(x.text),\n",
        "    sort_within_batch=True,\n",
        "    device = device)\n"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vy6C2FE0MdhB"
      },
      "source": [
        "import torch.nn as nn\n",
        "class FakeNewsClassifier(nn.Module):\n",
        "    \n",
        "    #define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        \n",
        "        #Constructor\n",
        "        super().__init__()          \n",
        "        \n",
        "        #embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        #lstm layer\n",
        "        self.lstm = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout,batch_first=True)\n",
        "        #dense layer\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        #activation function \n",
        "        # Uncomment if Loss function is not BCEWithLogitsLoss\n",
        "        # self.act = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "      \n",
        "        #packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(),batch_first=True)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        #unpack sequence\n",
        "        # output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "        \n",
        "        #concat the final forward and backward hidden state\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "                \n",
        "        #hidden = [batch size, hid dim * num directions]\n",
        "        dense_outputs=self.fc(hidden)\n",
        "\n",
        "        #Final activation function\n",
        "        # Uncomment if Loss function is not BCEWithLogitsLoss\n",
        "        # outputs=self.act(dense_outputs)\n",
        "        \n",
        "        return dense_outputs"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avKHPh9WNvWB"
      },
      "source": [
        "#define hyperparameters\n",
        "size_of_vocab = len(TEXT.vocab)\n",
        "embedding_dim = 100 \n",
        "# 100\n",
        "num_hidden_nodes = 32\n",
        "num_output_nodes = 1\n",
        "num_layers = 2\n",
        "bidirection = True\n",
        "dropout =  0.5\n",
        "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "#instantiate the model\n",
        "model = FakeNewsClassifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers, \n",
        "                   bidirectional = True, dropout = dropout, pad_idx=pad_idx)"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCnze4lpN5kC",
        "outputId": "74b91263-7595-4a22-a0fd-bc049cbdf8a2"
      },
      "source": [
        "#architecture\n",
        "print(model)\n",
        "#No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The LSTM model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "#Initialize the pretrained embedding\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FakeNewsClassifier(\n",
            "  (embedding): Embedding(25002, 100, padding_idx=1)\n",
            "  (lstm): LSTM(100, 32, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n",
            "The LSTM model has 2,559,657 trainable parameters\n",
            "torch.Size([25002, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAlm6oq0beHB"
      },
      "source": [
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(embedding_dim)\n",
        "model.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4y8YDfAOUNb"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "#define optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "#define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    # #round predictions to the closest in\n",
        "    # rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    \n",
        "    # correct = (rounded_preds == y).float() \n",
        "    # acc = correct.sum() / len(correct)\n",
        "    # return acc\n",
        "    precision,recall,fscore,support=score(y,preds,average='micro')\n",
        "    return fscore\n",
        "    \n",
        "#push to cuda if available\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA8f-7p8OeSM"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    y_pred = []\n",
        "    y_true = []   \n",
        "    threshold=0.5\n",
        "\n",
        "    #initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    #set the model in training phase\n",
        "    model.train()  \n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        #resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        #retrieve text and no. of words\n",
        "        text, text_lengths = batch.text   \n",
        "        labels = batch.label\n",
        "        #convert to 1D tensor\n",
        "        predictions = model(text, text_lengths).squeeze(1)  \n",
        "        # predictions = model(batch.text).squeeze(1)  \n",
        "        \n",
        "        #compute the loss\n",
        "        loss = criterion(predictions, batch.label)        \n",
        "        \n",
        "        #compute the binary accuracy\n",
        "        # acc = binary_accuracy(predictions, batch.label)   \n",
        "        output = (predictions > threshold).int()\n",
        "        y_pred.extend(output.tolist())\n",
        "        y_true.extend(labels.tolist())        \n",
        "        #backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        #update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        #loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        # epoch_acc += acc.item()    \n",
        "    epoch_acc = binary_accuracy(y_pred, y_true) \n",
        "    return epoch_loss / len(iterator), epoch_acc"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRHpi76GO0yt"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    y_pred = []\n",
        "    y_true = []    \n",
        "    threshold=0.5\n",
        "\n",
        "    #initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    #deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    #deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            #retrieve text and no. of words\n",
        "            text, text_lengths = batch.text\n",
        "            labels = batch.label\n",
        "            #convert to 1d tensor\n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "            # predictions = model(batch.text).squeeze(1)\n",
        "            \n",
        "            #compute loss and accuracy\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            # acc = binary_accuracy(predictions, batch.label)\n",
        "            output = (predictions > threshold).int()\n",
        "            y_pred.extend(output.tolist())\n",
        "            y_true.extend(labels.tolist())        \n",
        "\n",
        "            #keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            # epoch_acc += acc.item()\n",
        "    epoch_acc = binary_accuracy(y_pred, y_true)     \n",
        "    return epoch_loss / len(iterator), epoch_acc"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T14YrEEqO5he",
        "outputId": "2226c7f9-6dd0-4fbf-834e-35076e5647fc"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "best_valid_loss = float('inf')\n",
        "best_valid_acc = float('inf')\n",
        "best_train_acc = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    #train the model\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "    #evaluate the model\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    #evaluate the model\n",
        "    # valid_loss, valid_acc = train(model, valid_iterator, optimizer, criterion)\n",
        "\n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        best_valid_acc = valid_acc\n",
        "        best_train_acc = train_acc\n",
        "        torch.save(model.state_dict(), data_path + 'saved_fnn_weights_100d.pt')\n",
        "    \n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.677 | Train Acc: 53.10%\n",
            "\t Val. Loss: 0.635 |  Val. Acc: 58.81%\n",
            "\tTrain Loss: 0.625 | Train Acc: 62.02%\n",
            "\t Val. Loss: 0.597 |  Val. Acc: 65.81%\n",
            "\tTrain Loss: 0.593 | Train Acc: 65.92%\n",
            "\t Val. Loss: 0.591 |  Val. Acc: 62.79%\n",
            "\tTrain Loss: 0.555 | Train Acc: 68.79%\n",
            "\t Val. Loss: 0.605 |  Val. Acc: 68.77%\n",
            "\tTrain Loss: 0.532 | Train Acc: 70.69%\n",
            "\t Val. Loss: 0.581 |  Val. Acc: 67.92%\n",
            "\tTrain Loss: 0.488 | Train Acc: 74.58%\n",
            "\t Val. Loss: 0.622 |  Val. Acc: 68.44%\n",
            "\tTrain Loss: 0.447 | Train Acc: 77.49%\n",
            "\t Val. Loss: 0.646 |  Val. Acc: 69.66%\n",
            "\tTrain Loss: 0.392 | Train Acc: 80.86%\n",
            "\t Val. Loss: 0.708 |  Val. Acc: 67.92%\n",
            "\tTrain Loss: 0.366 | Train Acc: 82.76%\n",
            "\t Val. Loss: 0.671 |  Val. Acc: 67.72%\n",
            "\tTrain Loss: 0.329 | Train Acc: 84.87%\n",
            "\t Val. Loss: 0.776 |  Val. Acc: 68.74%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "Z9vputH9gLRZ",
        "outputId": "e5ebb159-f94f-45a8-b804-49cf24f842a3"
      },
      "source": [
        "# Test Function\n",
        "\n",
        "def test(model, iterator, threshold=0.5):\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "        \n",
        "            #retrieve text and no. of words\n",
        "            text, text_lengths = batch.text\n",
        "            labels = batch.label\n",
        "\n",
        "            #convert to 1d tensor\n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "\n",
        "            output = (predictions > threshold).int()\n",
        "            y_pred.extend(output.tolist())\n",
        "            y_true.extend(labels.tolist())\n",
        "    \n",
        "    print('Classification Report:')\n",
        "    print(classification_report(y_true, y_pred, labels=[0,1], digits=4))\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
        "    ax= plt.subplot()\n",
        "    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
        "\n",
        "    ax.set_title('Confusion Matrix')\n",
        "\n",
        "    ax.set_xlabel('Predicted Labels')\n",
        "    ax.set_ylabel('True Labels')\n",
        "\n",
        "    ax.xaxis.set_ticklabels(['FAKE', 'REAL'])\n",
        "    ax.yaxis.set_ticklabels(['FAKE', 'REAL'])\n",
        "    \n",
        "    \n",
        "path= data_path + 'saved_fnn_weights_100d.pt'\n",
        "model.load_state_dict(torch.load(path));\n",
        "test(model, test_iterator)"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9644    0.7657    0.8536       636\n",
            "           1     0.7286    0.9569    0.8273       418\n",
            "\n",
            "    accuracy                         0.8416      1054\n",
            "   macro avg     0.8465    0.8613    0.8405      1054\n",
            "weighted avg     0.8709    0.8416    0.8432      1054\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}